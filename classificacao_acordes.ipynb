{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LQXGYCoIdik"
      },
      "source": [
        "# Parte 1: Importa√ß√£o dos Arquivos\n",
        "\n",
        "Nesta etapa, realizamos a extra√ß√£o da pasta do arquivo `.zip`.  \n",
        "**Observa√ß√£o:** Antes, √© necess√°rio fazer o upload do arquivo `.zip` no Colab.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUBzhFg2Ii_E"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "\n",
        "# Nome do arquivo zip (mude se for diferente)\n",
        "zip_path = \"data\\Acordes.zip\"\n",
        "\n",
        "# Caminho de destino para extra√ß√£o\n",
        "extract_path = \"data\"\n",
        "\n",
        "# Extraindo\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCaqkVFTUYjW"
      },
      "source": [
        "# Parte 2: Extra√ß√£o de Atributos\n",
        "\n",
        "Nesta etapa, utilizamos a biblioteca **Librosa**, especializada em processamento de √°udio, para extrair atributos relevantes.\n",
        "\n",
        "Documenta√ß√£o de refer√™ncia:  \n",
        "https://librosa.org/doc/latest/generated/librosa.feature.mfcc.html#librosa.feature.mfcc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmqTygTocm0W",
        "outputId": "84f0e9aa-4d16-4dae-83f6-20497ebd0728"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Configura√ß√£o inicial\n",
        "pasta = 'data/Acordes/Acordes'\n",
        "lista_arquivos = os.listdir(pasta)\n",
        "arquivos_audio = []\n",
        "\n",
        "# Filtra arquivos .wav de acordes maiores/menores\n",
        "for arquivo in lista_arquivos:\n",
        "    if (arquivo.startswith('Major') or arquivo.startswith('Minor')) and arquivo.endswith('.wav'):\n",
        "        arquivos_audio.append(os.path.join(pasta, arquivo))\n",
        "\n",
        "dataset = []\n",
        "\n",
        "for filepath in arquivos_audio:\n",
        "    y, sr = librosa.load(filepath, sr=None)\n",
        "    \n",
        "    # --- Atributos temporais ---\n",
        "    rms = librosa.feature.rms(y=y).mean()\n",
        "    \n",
        "    # --- Atributos espectrais ---\n",
        "    # Chroma (prioridade m√°xima para acordes)\n",
        "    chroma_stft = librosa.feature.chroma_stft(y=y, sr=sr)\n",
        "    chroma_stft_mean = chroma_stft.mean()\n",
        "    chroma_stft_std = chroma_stft.std()\n",
        "    \n",
        "    chroma_cqt = librosa.feature.chroma_cqt(y=y, sr=sr)\n",
        "    chroma_cqt_mean = chroma_cqt.mean()\n",
        "    chroma_cqt_std = chroma_cqt.std()\n",
        "    \n",
        "    # Tonnetz (rela√ß√µes harm√¥nicas)\n",
        "    tonnetz = librosa.feature.tonnetz(y=librosa.effects.harmonic(y), sr=sr)\n",
        "    tonnetz_mean = tonnetz.mean(axis=1)\n",
        "    \n",
        "    # MFCCs (timbre)\n",
        "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=5)\n",
        "    mfcc_means = [mfcc[i].mean() for i in range(3)]  # Pegamos apenas os 3 primeiros coeficientes\n",
        "    \n",
        "    # --- Novos atributos sugeridos ---\n",
        "    # Raz√£o harm√¥nica/percussiva\n",
        "    y_harmonic, y_percussive = librosa.effects.hpss(y)\n",
        "    harmonic_ratio = np.mean(y_harmonic ** 2) / (np.mean(y_harmonic ** 2) + np.mean(y_percussive ** 2))\n",
        "    \n",
        "    # --- Classe ---\n",
        "    filename = os.path.basename(filepath)\n",
        "    classe = 'major' if filename.startswith('Major') else 'minor'\n",
        "    \n",
        "    # Armazena os atributos\n",
        "    data = {\n",
        "        'filename': filename,\n",
        "        'rms': rms,\n",
        "        'chroma_stft_mean': chroma_stft_mean,\n",
        "        'chroma_stft_std': chroma_stft_std,\n",
        "        'chroma_cqt_mean': chroma_cqt_mean,\n",
        "        'chroma_cqt_std': chroma_cqt_std,\n",
        "        'tonnetz_t1': tonnetz_mean[0],\n",
        "        'tonnetz_t2': tonnetz_mean[1],\n",
        "        'tonnetz_t3': tonnetz_mean[2],\n",
        "        'tonnetz_t4': tonnetz_mean[3],\n",
        "        'tonnetz_t5': tonnetz_mean[4],\n",
        "        'tonnetz_t6': tonnetz_mean[5],\n",
        "        'mfcc1': mfcc_means[0],\n",
        "        'mfcc2': mfcc_means[1],\n",
        "        'mfcc3': mfcc_means[2],\n",
        "        'harmonic_ratio': harmonic_ratio,\n",
        "        'class': classe\n",
        "    }\n",
        "    \n",
        "    dataset.append(data)\n",
        "\n",
        "# Converte para DataFrame\n",
        "df = pd.DataFrame(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Parte 3: An√°lise Explorat√≥ria\n",
        "\n",
        "Realizamos duas an√°lises importantes:\n",
        "\n",
        "1. **Matriz de Correla√ß√£o:** para identificar rela√ß√µes entre as features extra√≠das.\n",
        "2. **Boxplots:** para visualizar a distribui√ß√£o das principais features por classe (major ou minor).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- An√°lise Explorat√≥ria ---\n",
        "\n",
        "# 1. Matriz de Correla√ß√£o entre Features\n",
        "plt.figure(figsize=(15, 10))\n",
        "corr = df.drop(['filename', 'class'], axis=1).corr()\n",
        "sns.heatmap(corr, annot=True, cmap='coolwarm', center=0, fmt='.2f')\n",
        "plt.title('Matriz de Correla√ß√£o entre Features')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 2. Boxplots por Classe para Features Principais\n",
        "features_principais = ['chroma_stft_mean', 'chroma_cqt_mean', 'tonnetz_t1', 'mfcc1', 'harmonic_ratio']\n",
        "\n",
        "# Lista completa de features\n",
        "features_completas = [\n",
        "    'chroma_stft_mean', 'chroma_stft_std',\n",
        "    'chroma_cqt_mean', 'chroma_cqt_std',\n",
        "    'tonnetz_t1', 'tonnetz_t2', 'tonnetz_t3',\n",
        "    'tonnetz_t4', 'tonnetz_t5', 'tonnetz_t6',\n",
        "    'mfcc1', 'mfcc2', 'mfcc3',\n",
        "    'harmonic_ratio'\n",
        "]\n",
        "\n",
        "plt.figure(figsize=(20, 25))  # Aumentamos o tamanho da figura\n",
        "\n",
        "# Calculamos o n√∫mero necess√°rio de linhas (arredondando para cima)\n",
        "n_features = len(features_completas)\n",
        "n_cols = 3\n",
        "n_rows = (n_features + n_cols - 1) // n_cols  # Arredondamento para cima\n",
        "\n",
        "for i, feature in enumerate(features_completas, 1):\n",
        "    plt.subplot(n_rows, n_cols, i)  # Agora a grid se ajusta ao n√∫mero de features\n",
        "    sns.boxplot(x='class', y=feature, data=df)\n",
        "    plt.title(f'Distribui√ß√£o de {feature}')\n",
        "    \n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Parte 4: Normaliza√ß√£o dos Dados\n",
        "\n",
        "Antes de treinar o modelo, realizamos a **normaliza√ß√£o z-score** das features para garantir que todas estejam na mesma escala.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "\n",
        "# Caminho dos arquivos\n",
        "pasta = \"data/Acordes/Acordes\"  # substitua pelo seu caminho real\n",
        "lista_arquivos = os.listdir(pasta)\n",
        "arquivos_audio = []\n",
        "\n",
        "# Filtrar arquivos que s√£o maior ou menor\n",
        "for arquivo in lista_arquivos:\n",
        "    if (arquivo.startswith('Major') or arquivo.startswith('Minor')) and arquivo.endswith('.wav'):\n",
        "        arquivos_audio.append(os.path.join(pasta, arquivo))\n",
        "\n",
        "dataset = []\n",
        "\n",
        "for filepath in arquivos_audio:\n",
        "    y, sr = librosa.load(filepath, sr=None)  # Carrega o arquivo com librosa para extrair features avan√ßadas\n",
        "    \n",
        "    # --- Extra√ß√£o de todas as features ---\n",
        "    # Chroma STFT\n",
        "    chroma_stft = librosa.feature.chroma_stft(y=y, sr=sr)\n",
        "    chroma_stft_mean = np.mean(chroma_stft)\n",
        "    chroma_stft_std = np.std(chroma_stft)\n",
        "    \n",
        "    # Chroma CQT\n",
        "    chroma_cqt = librosa.feature.chroma_cqt(y=y, sr=sr)\n",
        "    chroma_cqt_mean = np.mean(chroma_cqt)\n",
        "    chroma_cqt_std = np.std(chroma_cqt)\n",
        "    \n",
        "    # Tonnetz\n",
        "    tonnetz = librosa.feature.tonnetz(y=librosa.effects.harmonic(y), sr=sr)\n",
        "    tonnetz_mean = tonnetz.mean(axis=1)  # M√©dia por coeficiente\n",
        "    \n",
        "    # MFCCs (pegando os 3 primeiros coeficientes)\n",
        "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=5)\n",
        "    mfcc1 = np.mean(mfcc[0])\n",
        "    mfcc2 = np.mean(mfcc[1])\n",
        "    mfcc3 = np.mean(mfcc[2])\n",
        "    \n",
        "    # Raz√£o harm√¥nica/percussiva\n",
        "    y_harmonic, y_percussive = librosa.effects.hpss(y)\n",
        "    harmonic_ratio = np.mean(y_harmonic ** 2) / (np.mean(y_harmonic ** 2) + np.mean(y_percussive ** 2))\n",
        "    \n",
        "    # RMS (Energia do sinal)\n",
        "    rms = librosa.feature.rms(y=y).mean()\n",
        "    \n",
        "    filename = os.path.basename(filepath)\n",
        "    classe = '1' if filename.startswith('Major') else '0'\n",
        "\n",
        "    dataset.append({\n",
        "        'filename': filename,\n",
        "        'class': classe,\n",
        "        'chroma_stft_mean': chroma_stft_mean,\n",
        "        'chroma_stft_std': chroma_stft_std,\n",
        "        'chroma_cqt_mean': chroma_cqt_mean,\n",
        "        'chroma_cqt_std': chroma_cqt_std,\n",
        "        'tonnetz_t1': tonnetz_mean[0],\n",
        "        'tonnetz_t2': tonnetz_mean[1],\n",
        "        'tonnetz_t3': tonnetz_mean[2],\n",
        "        'tonnetz_t4': tonnetz_mean[3],\n",
        "        'tonnetz_t5': tonnetz_mean[4],\n",
        "        'tonnetz_t6': tonnetz_mean[5],\n",
        "        'mfcc1': mfcc1,\n",
        "        'mfcc2': mfcc2,\n",
        "        'mfcc3': mfcc3,\n",
        "        'harmonic_ratio': harmonic_ratio,\n",
        "        'rms': rms\n",
        "    })\n",
        "\n",
        "# Criar DataFrame\n",
        "df = pd.DataFrame(dataset)\n",
        "\n",
        "# Lista com todas as features (excluindo filename e class)\n",
        "features_completas = [\n",
        "    'chroma_stft_mean', 'chroma_stft_std',\n",
        "    'chroma_cqt_mean', 'chroma_cqt_std',\n",
        "    'tonnetz_t1', 'tonnetz_t2', 'tonnetz_t3',\n",
        "    'tonnetz_t4', 'tonnetz_t5', 'tonnetz_t6',\n",
        "    'mfcc1', 'mfcc2', 'mfcc3',\n",
        "    'harmonic_ratio',\n",
        "    'rms'\n",
        "]\n",
        "\n",
        "# Normaliza√ß√£o z-score\n",
        "scaler = StandardScaler()\n",
        "df_normalizado = df.copy()\n",
        "df_normalizado[features_completas] = scaler.fit_transform(df[features_completas])\n",
        "\n",
        "# Exibir os primeiros registros para confer√™ncia\n",
        "print(\"DataFrame com todas as features normalizadas:\")\n",
        "print(df_normalizado.head())\n",
        "\n",
        "# Mostrar estat√≠sticas descritivas\n",
        "print(\"\\nEstat√≠sticas descritivas:\")\n",
        "print(df_normalizado[features_completas].describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Parte 5: Sele√ß√£o de Features com Random Forest\n",
        "\n",
        "Aplicamos o algoritmo **Random Forest** para avaliar a import√¢ncia de cada feature extra√≠da, identificando as mais relevantes para classifica√ß√£o.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Preparando os dados\n",
        "X = df_normalizado[features_completas]\n",
        "y = df_normalizado['class'].astype(int)  # Convertendo para num√©rico\n",
        "\n",
        "# Dividindo em treino e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Criando e treinando o modelo Random Forest\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Obtendo as import√¢ncias das features\n",
        "importances = rf.feature_importances_\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': features_completas,\n",
        "    'Importance': importances\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "# Visualizando as import√¢ncias\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='Importance', y='Feature', data=feature_importance_df, palette='viridis')\n",
        "plt.title('Feature Importance - Random Forest')\n",
        "plt.xlabel('Importance Score')\n",
        "plt.ylabel('Features')\n",
        "plt.show()\n",
        "\n",
        "# Avaliando o modelo\n",
        "y_pred = rf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Acur√°cia do Random Forest: {accuracy:.4f}\")\n",
        "\n",
        "# Selecionando as 3 melhores features\n",
        "features_principais = feature_importance_df['Feature'].head(2).tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Parte 6: K-Fold\n",
        "\n",
        "Implementamos a t√©cnica de **valida√ß√£o cruzada k-fold**, garantindo uma avalia√ß√£o robusta do modelo.  \n",
        "O conjunto de dados √© dividido estratificadamente em 10 folds.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def create_folds(df, k=10, seed=42):\n",
        "    np.random.seed()\n",
        "\n",
        "    # Obt√©m os √≠ndices das amostras para cada classe\n",
        "    index_menor = df[df['class'] == '0'].index.to_numpy()\n",
        "    index_maior = df[df['class'] == '1'].index.to_numpy()\n",
        "\n",
        "    # Embaralha os √≠ndices\n",
        "    np.random.shuffle(index_menor)\n",
        "    np.random.shuffle(index_maior)\n",
        "\n",
        "    # Divide os √≠ndices em k partes (folds) de forma sequencial\n",
        "    folds_menor = np.array_split(index_menor, k)\n",
        "    folds_maior = np.array_split(index_maior, k)\n",
        "\n",
        "    folds = []\n",
        "    for i in range(k):\n",
        "        fold = np.concatenate([folds_menor[i], folds_maior[i]])\n",
        "        np.random.shuffle(fold)\n",
        "        folds.append(fold)\n",
        "\n",
        "    return folds\n",
        "\n",
        "# Exemplo de uso:\n",
        "folds = create_folds(df_normalizado)\n",
        "\n",
        "for i, fold in enumerate(folds):\n",
        "    print(f\"Fold {i+1}: {fold}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Parte 7: Preditor KNN\n",
        "\n",
        "Desenvolvemos um **classificador KNN (K-Nearest Neighbors)** manual, com as seguintes etapas:\n",
        "\n",
        "- C√°lculo das dist√¢ncias euclidianas.\n",
        "- Sele√ß√£o dos k vizinhos mais pr√≥ximos.\n",
        "- Predi√ß√£o com base na maioria das classes.\n",
        "- Avalia√ß√£o de desempenho (TP, FP e acur√°cia).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def knn_predict(df, train_idx, val_idx, features, k=3):\n",
        "    \"\"\"\n",
        "    Realiza predi√ß√£o KNN para um fold de valida√ß√£o dado:\n",
        "    - df: DataFrame com dados\n",
        "    - train_idx: √≠ndices do conjunto de treino\n",
        "    - val_idx: √≠ndices do conjunto de valida√ß√£o\n",
        "    - features: lista com os nomes das colunas dos atributos\n",
        "    - k: n√∫mero de vizinhos\n",
        "    Retorna o n√∫mero de verdadeiros positivos, falsos positivos e acur√°cia.\n",
        "    \"\"\"\n",
        "    TP = 0\n",
        "    FP = 0\n",
        "\n",
        "    for idx_val in val_idx:\n",
        "        val_vector = df.loc[idx_val, features].to_numpy()\n",
        "        val_class = df.loc[idx_val, 'class']\n",
        "\n",
        "        # Calcula dist√¢ncias para todos os de treino\n",
        "        distances = []\n",
        "        for idx_train in train_idx:\n",
        "            train_vector = df.loc[idx_train, features].to_numpy()\n",
        "            dist = euclidean_distance(val_vector, train_vector)\n",
        "            train_class = df.loc[idx_train, 'class']\n",
        "            distances.append((dist, train_class))\n",
        "\n",
        "        # Ordena as dist√¢ncias\n",
        "        distances.sort(key=lambda x: x[0])\n",
        "\n",
        "        # Pega os k vizinhos mais pr√≥ximos\n",
        "        k_nearest = distances[:k]\n",
        "\n",
        "        # Conta classes entre os vizinhos\n",
        "        classes = [c for _, c in k_nearest]\n",
        "        pred_class = max(set(classes), key=classes.count)\n",
        "\n",
        "        # Verifica acerto\n",
        "        if pred_class == val_class:\n",
        "            TP += 1\n",
        "        else:\n",
        "            FP += 1\n",
        "\n",
        "    accuracy = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
        "    return TP, FP, accuracy\n",
        "\n",
        "# Dist√¢ncia Euclidiana\n",
        "def euclidean_distance(vec1, vec2):\n",
        "    return np.sqrt(np.sum((vec1 - vec2) ** 2))\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def run_kfold_knn(df, features, k_folds=10, k_neighbors=3):\n",
        "    \"\"\"\n",
        "    Executa valida√ß√£o cruzada k-fold usando KNN.\n",
        "    \"\"\"\n",
        "    folds = create_folds(df, k=k_folds)\n",
        "    results = []\n",
        "\n",
        "    for i in range(k_folds):\n",
        "        val_idx = folds[i]\n",
        "        train_idx = np.concatenate([folds[j] for j in range(k_folds) if j != i])\n",
        "\n",
        "        TP, FP, acc = knn_predict(df, train_idx, val_idx, features, k=k_neighbors)\n",
        "        results.append({'fold': i+1, 'TP': TP, 'FP': FP, 'accuracy': acc})\n",
        "\n",
        "    return results\n",
        "\n",
        "def grid_search_knn(df, features, k_folds=10, k_values=[1, 3, 5, 7, 9]):\n",
        "    \"\"\"\n",
        "    Realiza busca em grade (grid search) sobre diferentes valores de k para o KNN.\n",
        "    \"\"\"\n",
        "    grid_results = []\n",
        "\n",
        "    for k in k_values:\n",
        "        fold_results = run_kfold_knn(df, features, k_folds=k_folds, k_neighbors=k)\n",
        "        accuracies = [res['accuracy'] for res in fold_results]\n",
        "        mean_accuracy = np.mean(accuracies)\n",
        "        \n",
        "        grid_results.append({\n",
        "            'k': k,\n",
        "            'mean_accuracy': mean_accuracy,\n",
        "            'all_accuracies': accuracies\n",
        "        })\n",
        "    \n",
        "    return grid_results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fazendo as Predi√ß√µes com o KNN Manual\n",
        "\n",
        "Utilizamos as **N melhores features** selecionadas via Random Forest para realizar as predi√ß√µes usando o KNN implementado manualmente.  \n",
        "Realizamos um **grid search** com diferentes valores de `k` (3, 5, 7 e 9).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Selecionando as N melhores features da feature importance\n",
        "features_principais = feature_importance_df['Feature'].head(3).tolist()\n",
        "\n",
        "# Rodando o grid search para k = 1, 3, 5, 7, 9\n",
        "resultados_grid = grid_search_knn(df_normalizado, features_principais, k_folds=10, k_values=[3, 5, 7, 9])\n",
        "\n",
        "for res in resultados_grid:\n",
        "    print(f\"k = {res['k']}, Acur√°cia m√©dia = {res['mean_accuracy']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comparando com o KNN do Scikit-Learn\n",
        "\n",
        "Para validar nossa implementa√ß√£o manual, comparamos os resultados com a vers√£o do **KNN** da biblioteca **Scikit-Learn** utilizando:\n",
        "\n",
        "- `GridSearchCV` para busca pelos melhores par√¢metros.\n",
        "- `KFold` com 10 divis√µes e embaralhamento.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV, KFold\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Separando features e target\n",
        "X = df_normalizado[features_principais].values\n",
        "y = df_normalizado['class'].values\n",
        "\n",
        "# Definindo o modelo KNN\n",
        "knn = KNeighborsClassifier()\n",
        "\n",
        "# Definindo os par√¢metros para o grid search\n",
        "param_grid = {'n_neighbors': [3, 5, 7, 9]}\n",
        "\n",
        "# Configurando o KFold com embaralhamento e seed\n",
        "cv = KFold(n_splits=10, shuffle=True)\n",
        "\n",
        "# Configurando o grid search com o KFold customizado\n",
        "grid_search = GridSearchCV(knn, param_grid, cv=cv, scoring='accuracy')\n",
        "\n",
        "# Ajustando o modelo\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Resultados\n",
        "for mean, params in zip(grid_search.cv_results_['mean_test_score'], grid_search.cv_results_['params']):\n",
        "    print(f\"k = {params['n_neighbors']}, Acur√°cia m√©dia = {mean:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ‚úÖ **Conclus√£o - Compara√ß√£o entre KNN Manual e Scikit-Learn**\n",
        "\n",
        "Com base nos resultados obtidos, podemos destacar os seguintes pontos importantes:\n",
        "\n",
        "- ‚úÖ **Desempenho:**  \n",
        "Ambas as implementa√ß√µes apresentaram **acur√°cias m√©dias bastante pr√≥ximas**, com pequenas flutua√ß√µes dependendo da execu√ß√£o e do valor de `k`.\n",
        "\n",
        "- ‚úÖ **Robustez:**  \n",
        "O **KNN manual** demonstrou desempenho competitivo, validando a correta implementa√ß√£o do algoritmo.  \n",
        "J√° o **KNN do Scikit-Learn**, apesar de ser uma \"caixa-preta\", √© altamente otimizado, garantindo resultados r√°pidos e eficientes.\n",
        "\n",
        "- ‚úÖ **Varia√ß√µes nas execu√ß√µes:**  \n",
        "As diferen√ßas entre as execu√ß√µes ocorrem principalmente devido √† **aleatoriedade na divis√£o dos folds** durante a valida√ß√£o cruzada, al√©m de pequenas nuances na implementa√ß√£o do c√°lculo das dist√¢ncias e sele√ß√£o dos vizinhos.\n",
        "\n",
        "- ‚úÖ **Efici√™ncia:**  \n",
        "A utiliza√ß√£o do **Scikit-Learn** se destaca pela **praticidade e velocidade**, sendo ideal para aplica√ß√µes profissionais e em larga escala.  \n",
        "Por outro lado, a **implementa√ß√£o manual** √© fundamental para fins educacionais, proporcionando uma **compreens√£o aprofundada** do funcionamento interno do algoritmo.\n",
        "\n",
        "---\n",
        "\n",
        "üéØ **Em resumo:**  \n",
        "A pr√°tica de desenvolver o KNN manualmente, comparando-o com a biblioteca Scikit-Learn, √© uma excelente abordagem para consolidar o entendimento te√≥rico e verificar na pr√°tica a efic√°cia das t√©cnicas de Machine Learning.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "acordesvenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
